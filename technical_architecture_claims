now tell me, if the column names of xlsx file or databrics table changed or increses, then does I need to do anything in code for calculation or it wiil automatically adjusted in calculation , any changes need to d from my side?10:19 AMGreat question. Let me trace exactly how column names flow through the entire system before answering.Read the KEY_MAP in claimsCalculator.jsRead the KEY_MAP in claimsCalculator.jsRead the KEY_MAP in claimsCalculator.jsDoneGood question. Let me give you a clear, honest answer based on exactly how the code works.

The Short Answer
It depends on which column changed. The system handles some changes automatically, but others require you to add one line to KEY_MAP in claimsCalculator.js.

How the Column Mapping Works
Every incoming row â€” whether from Excel or Databricks â€” passes through normalizeRow() first:
jsconst lk = rawKey.trim().toLowerCase().replace(/\s+/g, "_");
const canonical = KEY_MAP[lk] || lk;  // â† the critical fallback
The || lk fallback means: if the key is not in KEY_MAP, it passes through as-is (lowercased + underscored). This is the key to understanding what's automatic and what's not.

Scenario 1 â€” Column Renamed to Something Already in KEY_MAP âœ… Automatic
If your source renames a column but the new name is already a known variant in KEY_MAP, nothing breaks.
Example: APPROVEDAMOUNT renamed to Approved_Amount
js// Both already in KEY_MAP â†’ both resolve to "approved_amount"
"approvedamount":  "approved_amount",
"approved_amount": "approved_amount",
Zero code changes needed.

Scenario 2 â€” Column Renamed to Something NOT in KEY_MAP âš ï¸ Add 1 Line
If the new name is genuinely new (not a variant already listed), the || lk fallback keeps it as a passthrough key. The calculation that reads r.approved_amount won't find any value â€” it silently gets 0.
Example: Your Databricks team renames APPROVEDAMOUNT â†’ NET_PAID_AMOUNT
The normalizer produces net_paid_amount (not found in KEY_MAP), so approved_amount is never set. Every cost KPI goes to â‚±0.
Fix â€” one line in claimsCalculator.js:
js"net_paid_amount": "approved_amount",   // â† add this
That's it. No other file needs to change.

Scenario 3 â€” New Column Added to the Table âœ… Automatic (mostly)
If your source adds a brand new column that the system doesn't currently use (e.g. region, sub_branch, premium_amount), it passes through via the || lk fallback and sits quietly in the row object. All existing KPIs keep working perfectly â€” they only read the canonical keys they already know about. The new column is simply ignored.
If you later want to use that new column in a calculation, then you add it to KEY_MAP and write the logic. But nothing breaks automatically.

Scenario 4 â€” Core Identity Column Renamed âŒ Must Fix
The one column you absolutely cannot rename without a code change is the grouping key:
jsconst entity = String(row.entity || "Unknown").trim();
If Entity is renamed (e.g. to Company_Name or Client), the system can't group rows into companies â€” every row becomes "Unknown" and you get one giant merged client.
Fix:
js// KEY_MAP
"company_name": "entity",   // or whatever the new name is
"client":       "entity",
Currently entity, organization, company, and account are all already mapped. If your rename is one of those, you're fine.

Scenario 5 â€” isClaimsLevelData() Detection âš ï¸ Possible Silent Fallback
When the server boots, it runs this check to decide if the data is claim-level or pre-summarised:
jsconst signals = ["illness", "facility_type", "claim_no", "approved_amount",
  "icd_code", "member_id", "claim_type", "plan_level", "illness_group"];
return signals.filter(f => keys.includes(f)).length >= 3;
```

It needs to find **at least 3 of these 9 canonical keys** in the first row. If multiple critical columns are renamed to unknown names simultaneously, it might fall below 3 and the server incorrectly treats your data as pre-summarised â€” skipping all aggregation and showing raw rows instead.

You'd see this in the terminal:
```
[parser] ğŸ“Š Pre-summarised client data   â† wrong, means detection failed
```
instead of:
```
[parser] ğŸ” HMO claim-level data â€” aggregating by Entity...  â† correct
Fix: Same as always â€” add the new names to KEY_MAP first, and detection works again.

Summary Table
SituationAutomatic?Action RequiredColumn renamed to variant already in KEY_MAPâœ… YesNothingNew column added you don't needâœ… YesNothingColumn renamed to unknown new nameâŒ NoAdd 1 line to KEY_MAPEntity column renamed to unknown nameâŒ NoAdd 1 line to KEY_MAPNew column added you want to USE in KPIsâŒ NoAdd to KEY_MAP + write calcMany columns renamed at once (detection fails)âŒ NoAdd lines to KEY_MAP first

The One File You Ever Touch
backend/claimsCalculator.js â€” only the KEY_MAP object at lines 28â€“121.
Everything else (dataParser, databricksConnector, server, frontend) is completely untouched. The KEY_MAP is deliberately the single point of truth for all column name variations across both data sources.
Adding a new mapping takes 30 seconds:
js"your_new_column_name": "canonical_key",
```

And you can always check which column names the server actually sees by hitting:
```
GET http://localhost:3001/api/debug/columns
This shows the exact keys present in your loaded data after normalization, so you can verify your mapping worked.now what if other clients data comes with same column names?10:29 AMGood question. Let me check exactly how the current code handles multiple clients' data.Read the aggregation logic to see how entity grouping worksRead the aggregation logic to see how entity grouping worksRead the aggregation logic to see how entity grouping worksDoneThat's all I need to see. The answer is clear.

The Answer: It Already Works Automatically âœ…
The system is already designed for exactly this scenario. Multiple clients' data â€” from the same or different sources â€” all land in the same Excel sheet or Databricks table, and the Entity column is the only thing that matters.
Here's the entire grouping logic:
jsconst byEntity = {};
for (const row of claimRows) {
  const entity = String(row.entity || "Unknown").trim();
  if (!byEntity[entity]) byEntity[entity] = [];
  byEntity[entity].push(row);  // â† rows sorted into buckets by company name
}
```

Every row goes into a bucket named after its `Entity` value. Then **all 55 KPIs are calculated independently per bucket**. The companies never see each other's data at any point.

---

## Concrete Example

Say your Excel file has 50,000 rows across 3 companies:
```
Entity          | Claim_No | APPROVEDAMOUNT | Illness_Group  | ...
----------------|----------|----------------|----------------|----
Accenture PH    | ACN-001  | 12,500         | Digestive      |
BDO Unibank     | BDO-001  | 8,200          | Respiratory    |
Accenture PH    | ACN-002  | 45,000         | Cardiovascular |
Jollibee Corp   | JFC-001  | 3,100          | Dental         |
BDO Unibank     | BDO-002  | 91,000         | Neoplasms      |
```

The system produces exactly 3 completely isolated client objects:
- `Accenture PH` â€” PMPM, top diagnoses, risk score etc. calculated from only ACN rows
- `BDO Unibank` â€” calculated from only BDO rows
- `Jollibee Corp` â€” calculated from only JFC rows

No overlap, no contamination. The column names are identical across all three â€” that's perfectly fine, even expected.

---

## What You DO Need to Watch

There are only 2 things that can go wrong across multi-client data:

**1. The `Entity` value must be consistent per company**

If Accenture's rows say `Accenture PH` in some months and `ACCENTURE PHILIPPINES` in others, they split into two separate client cards instead of one.
```
"Accenture PH"           â†’ Client card 1
"ACCENTURE PHILIPPINES"  â†’ Client card 2  â† wrong, should be same
This is a data quality issue, not a code issue. The fix is standardising the Entity column in your source data â€” not in the code.
2. The Masked_Member_ID must be unique per person, not recycled across companies
Unique member count is calculated as:
jsconst memberIdSet = new Set(rows.map(r => r.member_id));
const members = memberIdSet.size;
Since rows are already separated by Entity before this runs, member IDs from different companies never mix. Even if BDO and Accenture both have a member with ID EMP-00123, they're in different buckets so it doesn't matter.

Summary
SituationHandled?Same column names across all clientsâœ… Already works â€” columns are shared structure10 clients in one Excel sheetâœ… Each gets isolated analytics automatically100 clients in Databricks tableâœ… Same â€” Entity grouping scales to any numberNew client added to same fileâœ… Shows up as new card on next reload/restartSame member ID in two different companiesâœ… Safe â€” bucketed before member countingSame Entity name spelled differentlyâŒ Splits into two cards â€” fix in source data
You don't need to touch the code at all when new client data is added. Just ensure the Entity column value is spelled consistently for each company throughout the file.
